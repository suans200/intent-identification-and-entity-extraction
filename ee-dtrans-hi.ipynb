{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9185769,"sourceType":"datasetVersion","datasetId":5552539}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import (\n     BertTokenizerFast,\n    BertForTokenClassification,\n    get_scheduler\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-24T07:17:30.868340Z","iopub.execute_input":"2024-08-24T07:17:30.868767Z","iopub.status.idle":"2024-08-24T07:17:34.563182Z","shell.execute_reply.started":"2024-08-24T07:17:30.868728Z","shell.execute_reply":"2024-08-24T07:17:34.562279Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/data-set1/indic-health-demo-main/Dataset/IHQID-WebMD/train.csv')[['question_hindi','disease_hindi','drug_hindi','treatment_hindi']]\ntest_df=pd.read_csv('/kaggle/input/data-set1/indic-health-demo-main/Dataset/IHQID-WebMD/test.csv')[['question_hindi','disease_hindi','drug_hindi','treatment_hindi']]","metadata":{"execution":{"iopub.status.busy":"2024-08-24T07:17:34.565361Z","iopub.execute_input":"2024-08-24T07:17:34.566262Z","iopub.status.idle":"2024-08-24T07:17:34.623326Z","shell.execute_reply.started":"2024-08-24T07:17:34.566210Z","shell.execute_reply":"2024-08-24T07:17:34.622356Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T07:17:34.624722Z","iopub.execute_input":"2024-08-24T07:17:34.625093Z","iopub.status.idle":"2024-08-24T07:17:34.639550Z","shell.execute_reply.started":"2024-08-24T07:17:34.625055Z","shell.execute_reply":"2024-08-24T07:17:34.638570Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                      question_hindi disease_hindi drug_hindi  \\\n0           निस्टेटिन किस लिए निर्धारित किया गया है?           NaN  निस्टेटिन   \n1  क्या सम्भोग के बाद डाउचिंग मुझे गर्भवती होने स...       गर्भवती        NaN   \n2           क्या पेर्कोसेट वजन बढ़ने का कारण बनता है     भार बढ़ना   पर्कोसेट   \n3  क्या एक दिन में शराब के २ या २ १/२ गिलास उच्च ...  उच्च रक्तचाप        NaN   \n4        क्या बहुत अधिक छाछ थ्रश का कारण बन सकती है?          थ्रश        NaN   \n\n  treatment_hindi  \n0             NaN  \n1             NaN  \n2             NaN  \n3             NaN  \n4             NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question_hindi</th>\n      <th>disease_hindi</th>\n      <th>drug_hindi</th>\n      <th>treatment_hindi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>निस्टेटिन किस लिए निर्धारित किया गया है?</td>\n      <td>NaN</td>\n      <td>निस्टेटिन</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>क्या सम्भोग के बाद डाउचिंग मुझे गर्भवती होने स...</td>\n      <td>गर्भवती</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>क्या पेर्कोसेट वजन बढ़ने का कारण बनता है</td>\n      <td>भार बढ़ना</td>\n      <td>पर्कोसेट</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>क्या एक दिन में शराब के २ या २ १/२ गिलास उच्च ...</td>\n      <td>उच्च रक्तचाप</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>क्या बहुत अधिक छाछ थ्रश का कारण बन सकती है?</td>\n      <td>थ्रश</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from deep_translator import GoogleTranslator\ngoogle_tranlate = GoogleTranslator(source='hi', target='en')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T07:17:34.642189Z","iopub.execute_input":"2024-08-24T07:17:34.642553Z","iopub.status.idle":"2024-08-24T07:17:34.776279Z","shell.execute_reply.started":"2024-08-24T07:17:34.642517Z","shell.execute_reply":"2024-08-24T07:17:34.775369Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"type(train_df['question_hindi'][0])","metadata":{"execution":{"iopub.status.busy":"2024-08-24T07:17:34.777701Z","iopub.execute_input":"2024-08-24T07:17:34.778317Z","iopub.status.idle":"2024-08-24T07:17:34.785866Z","shell.execute_reply.started":"2024-08-24T07:17:34.778270Z","shell.execute_reply":"2024-08-24T07:17:34.784925Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"str"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(len(train_df)):\n    if type(train_df.loc[i,'question_hindi']) is not float:\n        train_df.loc[i,'question_hindi']=google_tranlate.translate(train_df.loc[i,'question_hindi'])\n    if type(train_df.loc[i,'disease_hindi']) is not float:\n        train_df.loc[i,'disease_hindi']=google_tranlate.translate(train_df.loc[i,'disease_hindi'])\n    if type(train_df.loc[i,'drug_hindi']) is not float:\n        train_df.loc[i,'drug_hindi']=google_tranlate.translate(train_df.loc[i,'drug_hindi'])\n    if type(train_df.loc[i,'treatment_hindi']) is not float:\n        train_df.loc[i,'treatment_hindi']=google_tranlate.translate(train_df.loc[i,'treatment_hindi'])\nfor i in range(len(test_df)):\n    if type(test_df.loc[i,'question_hindi']) is not float:\n        test_df.loc[i,'question_hindi']=google_tranlate.translate(test_df.loc[i,'question_hindi'])\n    if type(test_df.loc[i,'disease_hindi']) is not float:\n        test_df.loc[i,'disease_hindi']=google_tranlate.translate(test_df.loc[i,'disease_hindi'])\n    if type(test_df.loc[i,'drug_hindi']) is not float:\n        test_df.loc[i,'drug_hindi']=google_tranlate.translate(test_df.loc[i,'drug_hindi'])\n    if type(test_df.loc[i,'treatment_hindi']) is not float:\n        test_df.loc[i,'treatment_hindi']=google_tranlate.translate(test_df.loc[i,'treatment_hindi'])\ntrain_df=train_df.rename(columns={'question_hindi':'question_english','disease_hindi':'disease_english','drug_hindi':'drug_english','treatment_hindi':'treatment_english'})\ntest_df=test_df.rename(columns={'question_hindi':'question_english','disease_hindi':'disease_english','drug_hindi':'drug_english','treatment_hindi':'treatment_english'})\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T07:17:34.787391Z","iopub.execute_input":"2024-08-24T07:17:34.787840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fuzzywuzzy import fuzz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_common_sequence(tokenized_sentence,tokenized_entity,entity,tag_list):\n    common_sequence={\n        'similarity':0.0,\n        'start_index':-1,\n        'end_index':-1\n    }\n    target_len=len(tokenized_entity)\n    sentence_len=len(tokenized_sentence)\n    for i in range(sentence_len-target_len):\n        fuzz_ratio=fuzz.ratio(tokenized_sentence[i:i+target_len],tokenized_entity)\n        if fuzz_ratio>=80 and common_sequence['similarity']<fuzz_ratio:\n            common_sequence['similarity']=fuzz_ratio\n            common_sequence['start_index']=i\n            common_sequence['end_index']=i+target_len-1\n        fuzz_ratio=fuzz.ratio(tokenized_sentence[i:i+target_len-1],tokenized_entity)\n        if fuzz_ratio>=80 and common_sequence['similarity']<fuzz_ratio:\n            common_sequence['similarity']=fuzz_ratio\n            common_sequence['start_index']=i;\n            common_sequence['end_index']=i+target_len-1\n    tag_list[common_sequence['start_index']]=\"B-\"+entity\n    for i in range(common_sequence['start_index']+1,common_sequence['end_index']):\n        tag_list[i]=\"I-\"+entity\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import  word_tokenize\nimport math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(train_df)):\n    tokenised_sentence=word_tokenize(train_df.loc[i,'question_english'].lower())\n    tag_list=['O' for i in tokenised_sentence]\n    if type(train_df.loc[i,'disease_english']) is not float:\n        tokenized_disease=[word_tokenize(j.lower()) for j in train_df.loc[i,'disease_english'].split(',')]\n        for k in tokenized_disease:\n            get_common_sequence(tokenised_sentence,k,\"disease\",tag_list)\n    else:\n        assert(math.isnan(train_df.loc[i,'disease_english']))\n    if type(train_df.loc[i,'drug_english']) is not float:\n        tokenized_disease=[word_tokenize(j.lower()) for j in train_df.loc[i,'drug_english'].split(',')]\n        for k in tokenized_disease:\n            get_common_sequence(tokenised_sentence,k,\"drug\",tag_list)\n    else:\n        assert(math.isnan(train_df.loc[i,'drug_english']))\n    if type(train_df.loc[i,'treatment_english']) is not float:\n        tokenized_disease=[word_tokenize(j.lower()) for j in train_df.loc[i,'treatment_english'].split(',')]\n        for k in tokenized_disease:\n            get_common_sequence(tokenised_sentence,k,\"treatment\",tag_list)\n    else:\n        assert(math.isnan(train_df.loc[i,'treatment_english']))\n    train_df.loc[i,'question_english']=str(tokenised_sentence)\n    train_df.loc[i,'tag_english']=str(tag_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(test_df)):\n    tokenised_sentence=word_tokenize(test_df.loc[i,'question_english'].lower())\n    tag_list=['O' for i in tokenised_sentence]\n    if type(test_df.loc[i,'disease_english']) is not float:\n        tokenized_disease=[word_tokenize(j.lower()) for j in test_df.loc[i,'disease_english'].split(',')]\n        for k in tokenized_disease:\n            get_common_sequence(tokenised_sentence,k,\"disease\",tag_list)\n    else:\n        assert(math.isnan(test_df.loc[i,'disease_english']))\n    if type(test_df.loc[i,'drug_english']) is not float:\n        tokenized_disease=[word_tokenize(j.lower()) for j in test_df.loc[i,'drug_english'].split(',')]\n        for k in tokenized_disease:\n            get_common_sequence(tokenised_sentence,k,\"drug\",tag_list)\n    else:\n        assert(math.isnan(test_df.loc[i,'drug_english']))\n    if type(test_df.loc[i,'treatment_english']) is not float:\n        tokenized_disease=[word_tokenize(j.lower()) for j in test_df.loc[i,'treatment_english'].split(',')]\n        for k in tokenized_disease:\n            get_common_sequence(tokenised_sentence,k,\"treatment\",tag_list)\n    else:\n        assert(math.isnan(test_df.loc[i,'treatment_english']))\n    test_df.loc[i,'question_english']=str(tokenised_sentence)\n    test_df.loc[i,'tag_english']=str(tag_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_string(string):\n    special_remove=string.replace('[','').replace(']','').replace('\"','').replace(\"'\", '')\n    split_str=special_remove.split(',')\n    return [item.strip() for item in split_str]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['question_english']=train_df['question_english'].apply(convert_string)\ntrain_df['tag_english']=train_df['tag_english'].apply(convert_string)\ntest_df['question_english']=test_df['question_english'].apply(convert_string)\ntest_df['tag_english']=test_df['tag_english'].apply(convert_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label__ = {\n    'O': 0,\n    'B-treatment': 1,\n    'I-treatment': 2,\n    'B-disease': 3,\n    'I-disease': 4,\n    'B-drug': 5,\n    'I-drug': 6\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice='cuda' if torch.cuda.is_available() else 'cpu' \ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n\nhyper_parameters = {\n    'batch_size': 8,\n    'lr': 3e-5,\n    'epochs': 10\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_list(lst):\n    return [item for item in lst if item.strip()]\ntrain_df['question_english']=train_df['question_english'].apply(clean_list)\ntest_df['question_english']=test_df['question_english'].apply(clean_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=BertTokenizerFast.from_pretrained(model_checkpoint,add_prefix_space=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def labelling(question,tag):\n#     question = \" \".join(question)\n#     tokenized_input = tokenizer(question, truncation=True, padding='max_length', is_split_into_words=False)\n#     word_ids = tokenized_input.word_ids()\n#     for i, labels in enumerate(word_ids):\n#         if labels is None:\n#             word_ids[i] = 7\n#         else:\n#             word_ids[i] = label_1[tag[labels]]\n#     tokenized_input[\"labels\"] = word_ids\n#     return tokenized_input\ndef process_queries(question, tag): \n    \n    tokenized_input = tokenizer(question, max_length=300, truncation=True, padding='max_length', is_split_into_words=True)\n    word_ids = tokenized_input.word_ids()\n    j=1;\n    for i, label in enumerate(word_ids):\n        if label is None:\n            word_ids[i] = 7\n        else:\n            word_ids[i] = label__[tag[label]]\n            \n    tokenized_input[\"labels\"] = word_ids\n    return tokenized_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_input_train = {\n    'input_ids': [],\n    'attention_mask': [],\n    'tags_english': []\n}\nfor index in range(len(train_df['question_english'])):\n    process_output = process_queries(train_df.loc[index,'question_english'], train_df.loc[index,'tag_english'])\n    encoded_input_train['input_ids'].append(process_output['input_ids'])\n    encoded_input_train['attention_mask'].append(process_output['attention_mask'])\n    encoded_input_train['tags_english'].append(process_output['labels'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_input_test = {\n    'input_ids': [],\n    'attention_mask': [],\n    'tags_english': []\n}\nfor index in range(len(test_df['question_english'])):\n    process_output = process_queries(test_df.loc[index,'question_english'], test_df.loc[index,'tag_english'])\n    encoded_input_test['input_ids'].append(process_output['input_ids'])\n    encoded_input_test['attention_mask'].append(process_output['attention_mask'])\n    encoded_input_test['tags_english'].append(process_output['labels'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom tqdm import tqdm\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    TensorDataset(\n        torch.tensor(encoded_input_train['input_ids']).to(device),\n        torch.tensor(encoded_input_train['attention_mask']).to(device),\n        torch.tensor(encoded_input_train['tags_english']).to(device)\n    ),\n    batch_size=hyper_parameters['batch_size']\n)\ntest_dataloader = DataLoader(\n    TensorDataset(\n        torch.tensor(encoded_input_test['input_ids']).to(device),\n        torch.tensor(encoded_input_test['attention_mask']).to(device),\n        torch.tensor(encoded_input_test['tags_english']).to(device)\n    ),\n    batch_size=hyper_parameters['batch_size']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained(\n    model_checkpoint,\n    num_labels=len(label__) + 1\n)\nmodel.to(device)\n\noptimizer = AdamW(\n    model.parameters(),\n    lr=hyper_parameters['lr']\n)\n\nlr_scheduler = get_scheduler(\n  \"linear\",\n  optimizer=optimizer,\n  num_warmup_steps=0,\n  num_training_steps=hyper_parameters['epochs'] * len(train_dataloader)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\n\nupdater = tqdm(range(hyper_parameters['epochs']))\nfor epoch in updater:\n    total_train_loss = 0.0\n    for batch in train_dataloader:\n        \n        optimizer.zero_grad()\n        inputs = {\n            'input_ids': batch[0],\n            'attention_mask': batch[1],\n            'labels': batch[2],\n        }\n        \n        outputs = model(**inputs)\n        \n        loss = outputs.loss\n        loss.backward()\n        \n        optimizer.step()\n        lr_scheduler.step()\n        \n        total_train_loss += loss.item()\n    \n    print(\"Epoch:\", epoch + 1, \" - Training Loss:\", round(total_train_loss / len(train_dataloader), 4))\n\n\n# Didn't have much time to implement early stopping. So, saving the model at the end of all epochs.\ntorch.save(model.state_dict(), f'ee_rob_en.model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nprediction = []\ngold_label = []\n\nextra_appended_tokens = 0\n\nfor indexer, batch in enumerate(test_dataloader):\n\n    inputs = {\n        'input_ids': batch[0],\n        'attention_mask': batch[1],\n        'labels': batch[2],\n    }\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n\n    gold_label_cpu = inputs['labels'].cpu().numpy()\n    logits_vector = outputs.logits.detach().cpu().numpy()\n\n    assert(len(gold_label_cpu) == len(logits_vector))\n\n    for index in range(len(logits_vector)):\n        prediction_vector = []\n        for iterator__ in logits_vector[index].argmax(axis=1):\n            if iterator__ != 7:\n                prediction_vector.append(iterator__)\n                prediction.append(iterator__)\n        \n        gold_label_vector = []\n        for iterator__ in gold_label_cpu[index]:\n            if iterator__ != 7:\n                gold_label_vector.append(iterator__)\n                gold_label.append(iterator__)\n        \n        # There are some cases (only observed once) when there was one mismatch in vector of gold label and prediction\n        # To overcome that, for each tokenized\n         while len(gold_label) < len(prediction):\n            extra_appended_tokens += 1\n            gold_label.append(0)\n        \n        while len(prediction) < len(gold_label):\n            extra_appended_tokens += 1\n            prediction.append(0)\n\nprint(\"Number of extra appended tokens : \", extra_appended_tokens)\nprint(classification_report(gold_label, prediction))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}